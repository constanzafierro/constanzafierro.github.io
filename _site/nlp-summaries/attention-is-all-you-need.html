<!DOCTYPE html>
<html lang="en"><link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@300&display=swap" rel="stylesheet">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Summary: Attention is all you need, the transformer architecture. | Constanza Fierro</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Summary: Attention is all you need, the transformer architecture." />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/nlp-summaries/attention-is-all-you-need" />
<meta property="og:url" content="http://localhost:4000/nlp-summaries/attention-is-all-you-need" />
<meta property="og:site_name" content="Constanza Fierro" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-04-18T19:00:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Summary: Attention is all you need, the transformer architecture." />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2020-04-18T19:00:00+01:00","datePublished":"2020-04-18T19:00:00+01:00","headline":"Summary: Attention is all you need, the transformer architecture.","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nlp-summaries/attention-is-all-you-need"},"url":"http://localhost:4000/nlp-summaries/attention-is-all-you-need"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Constanza Fierro" /></head>
<body><main class="page-content" aria-label="Content">
      <div class="wrapper"><div class="top-page-navigation code">
          <a class="home" href="http://localhost:4000/" title="Back to Index">&lt;&lt Index</a>
        </div>
      
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline"><span>Summary: Attention is all you need, the transformer architecture.</span></h1>
    <p class="post-meta"><span>
      <time class="dt-published" datetime="2020-04-18T19:00:00+01:00" itemprop="datePublished">Apr 18, 2020
      </time></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<blockquote>
  <p>Paper summary: Attention is all you need , Dec. 2017. (<a href="https://arxiv.org/abs/1706.03762">link</a>)</p>
</blockquote>

<h2 id="why-is-it-important">Why is it important?</h2>

<p>This is the paper that first introduced the transformer architecture, which allowed language models to be way bigger than before thanks to its capability of being easily parallelizable. Consequently, models such as BERT and GPT achieved far better results in diverse NLP tasks.</p>

<h2 id="what-does-it-propose">What does it propose?</h2>

<p>This work proposed a network architecture to perform neural machine translation (NMT). This new model is entirely based on the attention mechanism, contrary to the standard at that point of using recurrent networks with attention. The architecture was tested in two NMT tasks and it outperformed the best existent models, in addition to using less resources. Furthermore, the model was also successfully tested in a different task (english constituency parsing).</p>

<p>The inherently sequential nature of RNNs precludes parallelization within training examples, moreover the best RNNs architectures don’t rely solely on one or a couple of hidden states, but use attention to attend to the most relevant hidden states. That’s why the architecture presented in this paper is so relevant and impactful, it’s able to achieve better results getting rid of the sequentiality of RNNs.</p>

<h2 id="how-does-it-work">How does it work?</h2>

<p>The Transformer is an encoder-decoder architecture, the encoder corresponds to the left side of the image below (Figure 1) and the decoder to the right one. In this paper the authors introduced the multi-head self-attention layer and the positional encodings used in the architecture (details in the next 2 sections).</p>

<p>Essentially, token embeddings are added with their positional encoding and used as inputs in the encoder and decoder. The encoder is composed of a stack of N=6 layers, we can see one of such layers in Figure 1. The decoder is also composed of a stack of N=6 identical layers, which has the two sub-layers of the encoder but it inserts a third sub-layer first that performs a multi-head attention over the output. Each feed-forward and multi-head self-attention layer is followed by a residual connection and a layer normalization, thus the output of each sub-layer is \(\text{LayerNorm}(x+\text{SubLayer}(x))\).</p>

<p>Some extra details:</p>

<ul>
  <li>Byte pair encoding is used to define the tokens of the text.</li>
  <li>The feed forward network consists of two linear transformations with a ReLu in between.</li>
  <li>Regularizations:
    <ul>
      <li>Dropout on the output of each sub-layer (before it’s added and normalized).</li>
      <li>Label smoothing.</li>
    </ul>
  </li>
  <li>Beam search is used to generate the text.</li>
</ul>

<p>(for further explanations in these concepts you can check my <a href="https://cfierro94.github.io/nlp-deep-dive/attention-is-all-you-need">deep dive of this paper</a>)</p>

<figure>
<img src="../assets/img/nlp-summary-01/architecture.png" alt="Figure 1. The Transformer architecture." style="width:60%; display: block;margin-left: auto;margin-right: auto;" />
<figcaption style="text-align: center;">Figure 1. The Transformer architecture.</figcaption>
</figure>

<h3 id="positional-encoding">Positional Encoding</h3>

<p><strong>Motivation</strong>: Since there’s no recurrence, a positional encoding vector is added to the token embedding to inject information about its position in the text.</p>

<p>The \(PE(w_t)\), the positional encoding for the word \(w\) at position \(t\), is a vector of dimension \(d_{model}\) equal to the embedding dimension. We compute each dimension \(i\) of this vector as follows:</p>

\[PE_i(w_t) = \left\{\begin{array}{ll}      sin(k_j*t) &amp; \text{if} \quad i=2j \\      cos(k_j* t) &amp; \text{if} \quad i=2j+1 \\\end{array} \right. \\[20pt]\text{where,} \quad k_j = \frac{1}{10000^{2i/d_{model}}}\]

<p>Which give as,</p>

\[PE(w_t) = \begin{bmatrix}sin(k_0t)\\cos(k_0t)\\... \\sin(k_{d/2}t)\\cos(k_{d/2}t)\end{bmatrix}\]

<p>We can think of this as a bit representation of numbers, with each dimension of the vector as a bit, since each bit changes periodically and we can tell that one number is bigger than another because of the bits activated and their order. More of this intuition in <a href="https://kazemnejad.com/blog/transformer_architecture_positional_encoding/">this blog post</a>.</p>

<p>The authors showed that learned positional embeddings were as good as these in their test sets. But they think sin/cos is better because it could extrapolate better.</p>

<h3 id="attention-layer">Attention Layer</h3>

<p>Instead of just having one attention layer the authors found beneficial to linearly project the attention mechanism, thus performing multiple attentions in parallel (Figure 2). Each of the attention mechanisms is a scaled dot-product attention (explained below).</p>

<p>The intuition behind is that having just one attention will lead to average all the different aspects of the text, whereas when we do parallel attention we are able to look at each of these details separately (the subject, the intention, the action, etc).</p>

<figure>
<img src="../assets/img/nlp-summary-01/attentions.png" alt="Figure 2. Attention mechanisms of the Transformer." style="width:70%; display: block;margin-left: auto;margin-right: auto;" />
<figcaption style="text-align: center;">Figure 2. Attention mechanisms of the Transformer.</figcaption>
</figure>

<h4 id="scaled-dot-product-attention">Scaled Dot-Product Attention</h4>

<p>Attention is a function that takes a query and a set of key-value pairs as inputs, and computes a weighted sum of the values, where the weights are obtained from a compatibility function between the query and the corresponding key.</p>

<p>The specific attention used here, is called <em>scaled dot-product</em> because the compatibility function used is:</p>

\[\text{weight}(q,k_i) =\text{softmax}(\frac{q\cdot k_i}{\sqrt{d_k}}) \quad \text{where} \, q,k_i \in \!R^{d_k}\]

<p>The authors decided to use a dot-product attention over an additive attention because it is faster to compute and more space-efficient. But it has been shown to perform worse for larger dimensions of the input (\(d_k\)), thus they added the scaling factor \(\sqrt{d_k}\) to counteract the effect of the dot product getting too large (which they suspect is the problem).</p>

<h4 id="masked-multi-head-self-attention">Masked Multi-Head Self-Attention</h4>

<p>In training we don’t want to show the complete output sentence to our model, but instead we want to present the words one by one to not let extra information flow in the decoder. That’s why in Figure 2 we see a “Mask opt.” which refers to setting those vectors to -inf, making them 0 after the softmax. Figure 3 can help to understand how this affects the architecture overall.</p>

<figure>
<img src="../assets/img/nlp-summary-01/mask-transformer.png" alt="Figure 3. The Transformer architecture masking the output." style="width:70%; display: block;margin-left: auto;margin-right: auto;" />
<figcaption style="text-align: center;">Figure 3. The Transformer architecture masking the output.</figcaption>
</figure>

<h2 id="whats-next">What’s next!</h2>

<p>BERT and GPT are just some of the applications that the Transformer can have, but it has also been applied to <a href="http://openaccess.thecvf.com/content_cvpr_2018/html/Hu_Squeeze-and-Excitation_Networks_CVPR_2018_paper.html">images</a>, <a href="https://arxiv.org/abs/1710.10903]">graph networks</a>, <a href="https://arxiv.org/abs/1805.08318">GANs</a>, among others, achieving state-of-the-art results. It has also been useful to interpret a part of the models that use it (<a href="https://arxiv.org/abs/1910.05276">https://arxiv.org/abs/1910.05276</a>).</p>

  </div><a class="u-url" href="/nlp-summaries/attention-is-all-you-need" hidden></a>
</article>

      </div><div class="page-navigation code">
        
          <a class="home" href="http://localhost:4000/" title="Back to Index">&lt;&lt Index</a>
        
      </div>
      
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading"></h2>

    <div class="footer-col-wrapper">
      <ul class="flex-container">
        <li class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/cfierro94"><svg
        class="svg-icon">
        <use xlink:href="/assets/minima-social-icons.svg#github"></use>
      </svg> <span class="username"></span></a></li><li><a
      href="https://www.twitter.com/constanzafierro"><svg class="svg-icon">
        <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
      </svg> <span class="username"></span></a></li><li><a href="https://medium.com/@cfierro"><svg
        class="svg-icon" viewBox="0 0 50 50" width="16px" height="16px">
        <path
          d="M15 12A13 13 0 1015 38 13 13 0 1015 12zM35.5 13c-3.59 0-6.5 5.373-6.5 12 0 1.243.102 2.441.292 3.568.253 1.503.662 2.879 1.192 4.065.265.593.56 1.138.881 1.627.642.978 1.388 1.733 2.202 2.201C34.178 36.811 34.827 37 35.5 37s1.322-.189 1.933-.539c.814-.468 1.56-1.223 2.202-2.201.321-.489.616-1.034.881-1.627.53-1.185.939-2.562 1.192-4.065C41.898 27.441 42 26.243 42 25 42 18.373 39.09 13 35.5 13zM45.5 14c-.259 0-.509.173-.743.495-.157.214-.307.494-.448.833-.071.169-.14.353-.206.551-.133.395-.257.846-.37 1.343-.226.995-.409 2.181-.536 3.497-.063.658-.112 1.349-.146 2.065C43.017 23.499 43 24.241 43 25s.017 1.501.051 2.217c.033.716.082 1.407.146 2.065.127 1.316.31 2.501.536 3.497.113.498.237.948.37 1.343.066.198.135.382.206.551.142.339.292.619.448.833C44.991 35.827 45.241 36 45.5 36c1.381 0 2.5-4.925 2.5-11S46.881 14 45.5 14z" />
      </svg> <span class="username"></span></a></li><li><a href="https://www.strava.com/athletes/13290450"><svg
        viewBox="0 0 512 512" class="svg-icon">
        <path d="M120 288L232 56l112 232h-72l-40-96-40 96z" />
        <path d="M280 288l32 72 32-72h48l-80 168-80-168z" />
      </svg><span class="username"></span></a></li></ul></li>
      </ul>
    </div>

  </div>

</footer>
</body>

</html>
