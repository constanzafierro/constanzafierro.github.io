<!DOCTYPE html>
<html lang="en"><link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@300&display=swap" rel="stylesheet">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches | Constanza Fierro</title>
<meta name="generator" content="Jekyll v4.0.1" />
<meta property="og:title" content="Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/nlp-summaries/can-small-and-synthetic-benchmarks-drive-modeling-innovation" />
<meta property="og:url" content="http://localhost:4000/nlp-summaries/can-small-and-synthetic-benchmarks-drive-modeling-innovation" />
<meta property="og:site_name" content="Constanza Fierro" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-02-07T18:00:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2021-02-07T18:00:00+00:00","datePublished":"2021-02-07T18:00:00+00:00","headline":"Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/nlp-summaries/can-small-and-synthetic-benchmarks-drive-modeling-innovation"},"url":"http://localhost:4000/nlp-summaries/can-small-and-synthetic-benchmarks-drive-modeling-innovation"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Constanza Fierro" /></head>
<body><main class="page-content" aria-label="Content">
      <div class="wrapper"><div class="top-page-navigation code">
          <a class="home" href="http://localhost:4000/" title="Back to Index">&lt;&lt Index</a>
        </div>
      
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline"><span>Can Small and Synthetic Benchmarks Drive Modeling Innovation? A Retrospective Study of Question Answering Modeling Approaches</span></h1>
    <p class="post-meta"><span>
      <time class="dt-published" datetime="2021-02-07T18:00:00+00:00" itemprop="datePublished">Feb 7, 2021
      </time></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p><a href="https://arxiv.org/abs/1810.04805">LINK TO THE PAPER</a></p>

<blockquote>
  <p>Small and carefully designed synthetic benchmarks (not created through crowdsourcing üí∏) may be useful as benchmarks for driving the development of new modeling approaches, instead of needing large and naturally created datasets.</p>
</blockquote>

<h2 id="main-idea">Main idea</h2>
<p>We do need natural and big datasets for models that will be used in production, but for making modeling progress (i.e. to compare if an architecture is better than other) we could maybe use automatically generated datasets (through a database, or using some big text corpora).</p>

<p>They showed that many human-constructed benchmarks (NewsQA, NaturalQuestions, HotpotQA, ‚Ä¶) have high concurrence with SQuAD. They also constructed synthetic benchmarks that do not resemble natural language, yet have high concurrence with SQuAD, demonstrating that <u>naturalness</u> and <u>size</u> are not necessary for reflecting historical modeling improvements on SQuAD.</p>

<h2 id="concurrence">Concurrence</h2>
<p>Concurrence is used in the paper to compare datasets and to be able to say: if two datasets have high concurrence then we could compare modeling decision with either of them, e.g. SQuAD has high concurrence with \(X\), and \(X\) is way cheaper to produce so we might have achieved similar progress using dataset \(X\) as benchmark.</p>

<h4 id="definition">Definition</h4>
<p>Two datasets have high concurrence if they rank a set of models similarly, when train on the dataset train set and evaluated on the dataset test set.</p>

<p>Mathematically, concurrence is the correlation between the evaluations obtained in two different datasets for the same model.</p>

<p>They used 2 correlation functions:</p>
<ol>
  <li>Measures that both evaluations have a linear relation (\(\implies\) ideally, if one model is \(k\) times better than other in dataset 1 it should be \(m*k\) better in dataset 2, with \(m\) a constant for all the model evaluations).</li>
  <li>Measures that both rankings (order) are similar.</li>
</ol>

<h2 id="synthetic-datasets">Synthetic datasets</h2>
<h3 id="fuzzysyntheticqa">FuzzySyntheticQA</h3>
<ul>
  <li>Passage: random 150 tokens (from a token vocabulary).</li>
  <li>Answer: random token from the passage.</li>
  <li>Question: noisy cloze generation of the 10 token window of the answer token. Cloze means that the answer token is masked out, and it‚Äôs noisy because they applied noisy ‚Äúoperations‚Äù to  the window: token replacement + token permutation + word dropout.</li>
</ul>

<p><strong>Results.</strong> It has high concurrence with SQuAD on non-pretrained modeling approaches. This is non trivial, since existing synthetic benchmarks (<a href="https://arxiv.org/abs/1502.05698">bAbI task suite</a>) have low concurrence with SQuAD.</p>

<h3 id="wikidatasyntheticqa">WikidataSyntheticQA</h3>
<p>Motivation: build synthetic cloze benchmarks that do not resemble natural language, yet require some of the reasoning capabilities necessary to handle complex natural language phenomena in SQuAD.</p>

<p>Derived from Wikidata triples ([subject, predicate, object]). Wikidata contains entities (subject/object) that relate to each other (predicate). Each entity has a label (its main name) and aliases (alternative names). They picked one entity and all the entities related to that one, then they sampled 50 triples to use.</p>
<ul>
  <li>Passage: It‚Äôs the concatenation of all 50 triples (separated by ‚Äú.‚Äù) replacing the entity with the label or an alias. So to have a ‚Äútext‚Äù version of the triple.</li>
  <li>Answer: Random element of a triple.</li>
  <li>Question: the triple that contains the answer, with the answer element masked and randomly transforming it by:
    <ul>
      <li>Replacing the predicate with the inverse and then inverting the subject and the object, e.g.: <code class="highlighter-rouge">(Mae_C._Jemison, employer, [MASK])</code> \(\rightarrow\) <code class="highlighter-rouge">([MASK], employee, Mae_C._Jemison)</code>.</li>
      <li>Replace the unmasked entity with its hypernyms (e.g. NASA \(\rightarrow\) space agency).</li>
    </ul>
  </li>
</ul>

<h4 id="results">Results</h4>
<ul>
  <li>It has high concurrence with SQuAD.
-This result indicates that naturalness is not a necessary quality of benchmarks with high concurrence with SQuAD.</li>
  <li>They think that this high correlation may be caused by  the fact that a model needs to reason about.</li>
  <li>The Wikidata aliases provide great lexical complexity  to the benchmark, such that the benchmark is not trivially solvable through string pattern-matching (removing aliases from the generation procedure results in near-perfect performance from all modeling approaches).</li>
</ul>

<h1 class="post-thoughts"><span>Questions/Thoughts</span></h1>
<ul>
  <li>Did their synthetic benchmarks work well because they knew the SQuAD dataset and knew which capacities they needed to measure?
    <ul>
      <li>Do we generally know which capabilities we want to measure?</li>
    </ul>
  </li>
  <li>I think this is an interesting idea to directly prove capabilities of a model. E.g. if I want to prove that my model understands antonyms I can build a synthetic dataset that contains that and test it. It‚Äôs good because it‚Äôd be more specific and more controlled, I‚Äôm sure at what level the model is good. Naturally generated datasets are created by humans, from whatever they thought at the moment or  understood from the task that was asked from them.</li>
  <li>It‚Äôs important (and maybe difficult to measure) to create a dataset that contains the lexical properties that one wants, while not being trivially solvable.</li>
  <li>On the negative  side, benchmarks inspire problems/issues of current approaches. Having a manually created dataset wouldn‚Äôt create this inspiration.</li>
</ul>

  </div><a class="u-url" href="/nlp-summaries/can-small-and-synthetic-benchmarks-drive-modeling-innovation" hidden></a>
</article>

      </div><div class="page-navigation code">
        
          <a class="home" href="http://localhost:4000/" title="Back to Index">&lt;&lt Index</a>
        
          <span> &middot; </span>
          <a class="prev" href="http://localhost:4000/nlp-summaries/recipes-open-domain-chatbot" title="PREV: Recipes for building an open-domain chatbot">Recipes for building an open-domain chatbot &gt;&gt</a>
        
      </div>
      
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading"></h2>

    <div class="footer-col-wrapper">
      <ul class="flex-container">
        <li class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/cfierro94"><svg
        class="svg-icon">
        <use xlink:href="/assets/minima-social-icons.svg#github"></use>
      </svg> <span class="username"></span></a></li><li><a
      href="https://www.twitter.com/constanzafierro"><svg class="svg-icon">
        <use xlink:href="/assets/minima-social-icons.svg#twitter"></use>
      </svg> <span class="username"></span></a></li><li><a href="https://medium.com/@cfierro"><svg
        class="svg-icon" viewBox="0 0 50 50" width="16px" height="16px">
        <path
          d="M15 12A13 13 0 1015 38 13 13 0 1015 12zM35.5 13c-3.59 0-6.5 5.373-6.5 12 0 1.243.102 2.441.292 3.568.253 1.503.662 2.879 1.192 4.065.265.593.56 1.138.881 1.627.642.978 1.388 1.733 2.202 2.201C34.178 36.811 34.827 37 35.5 37s1.322-.189 1.933-.539c.814-.468 1.56-1.223 2.202-2.201.321-.489.616-1.034.881-1.627.53-1.185.939-2.562 1.192-4.065C41.898 27.441 42 26.243 42 25 42 18.373 39.09 13 35.5 13zM45.5 14c-.259 0-.509.173-.743.495-.157.214-.307.494-.448.833-.071.169-.14.353-.206.551-.133.395-.257.846-.37 1.343-.226.995-.409 2.181-.536 3.497-.063.658-.112 1.349-.146 2.065C43.017 23.499 43 24.241 43 25s.017 1.501.051 2.217c.033.716.082 1.407.146 2.065.127 1.316.31 2.501.536 3.497.113.498.237.948.37 1.343.066.198.135.382.206.551.142.339.292.619.448.833C44.991 35.827 45.241 36 45.5 36c1.381 0 2.5-4.925 2.5-11S46.881 14 45.5 14z" />
      </svg> <span class="username"></span></a></li><li><a href="https://www.strava.com/athletes/13290450"><svg
        viewBox="0 0 512 512" class="svg-icon">
        <path d="M120 288L232 56l112 232h-72l-40-96-40 96z" />
        <path d="M280 288l32 72 32-72h48l-80 168-80-168z" />
      </svg><span class="username"></span></a></li></ul></li>
      </ul>
    </div>

  </div>

</footer>
</body>

</html>
