I"∫<<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<blockquote>
  <p>The objective of this article is to understand the concepts on which the transformer architecture (<a href="https://arxiv.org/abs/1706.03762">Vaswani et. al</a>) is based on.<br />
If you want a general overview of the paper you can check the <a href="https://cfierro94.github.io/nlp-summaries/attention-is-all-you-need">summary</a>.</p>
</blockquote>

<p>Here I‚Äôm going to present a summary of:</p>

<ul>
  <li><a href="https://cfierro94.github.io/nlp-deep-dive/attention-is-all-you-need#byte-pair-encoding">Byte pair encoding</a></li>
  <li><a href="https://cfierro94.github.io/nlp-deep-dive/attention-is-all-you-need#beam-search">Beam search</a></li>
  <li><a href="https://cfierro94.github.io/nlp-deep-dive/attention-is-all-you-need#label-smoothing">Label smoothing</a></li>
  <li><a href="https://cfierro94.github.io/nlp-deep-dive/attention-is-all-you-need#dropout">Dropout</a></li>
  <li><a href="https://cfierro94.github.io/nlp-deep-dive/attention-is-all-you-need#layer-normalization">Layer Normalization</a></li>
</ul>

<h2 id="byte-pair-encoding">Byte Pair Encoding</h2>

<h4 id="context">Context</h4>

<p>This is an algorithm to define the tokens for which we‚Äôre going to learn vector embeddings. The simplest way to do this is consider each word and punctuation in the text as a token. The problem with this approach is that in testing we won‚Äôt have an embedding for words we didn‚Äôt see before. Some research has successfully used characters as tokens (<a href="https://arxiv.org/abs/1508.06615">Kim et. al 2016</a>, <a href="https://arxiv.org/abs/1603.00810">Costa-Jussa et. al 2016</a>). Byte pair encoding can be put in the middle of these two techniques.</p>

<h4 id="the-algorithm">The algorithm</h4>

<p>The motivation behind the algorithm is to define a set of tokens that can be used to construct any word, but also contain the most typical words. So we can learn good representations for the most common terms but at the same time remain flexible and have some knowledge for unknown words. The algorithm then is as follows:</p>

<ol>
  <li>We start the tokens set with each of the possible characters plus an end-word character.</li>
  <li>We determine the number of merges that we want to do.</li>
  <li>For every merge, we will count the occurences of each pair of tokens in our corpus, and we‚Äôre going to add as a string the pair (the concatenation) the most frequent. Therefore, adding 1 new token to our set.</li>
</ol>

<p>With this, the size of the vocabulary = the number of merges + the number of different characters + 1 (the end-word character). So if we define the number of merges as <script type="math/tex">\inf</script> then the vocabulary is all the possible characters and all the different words.</p>

<h2 id="beam-search">Beam Search</h2>

<h4 id="context-1">Context</h4>

<p>There are different algorithms to decode the final output sequence. This is because our model outputs a probability distribution over our vocabulary, and then we need to choose one word each time until we arrived at the end character. One option (<em>greddy decoding</em>) would be to choose at each step the word with the highest probability, but the problem is that this may not lead to the highest probability sentence because it‚Äôs calculated as:</p>

<script type="math/tex; mode=display">P(w_0...w_n) = \prod_i P(w_i) \\
\iff \log(P(w_0...w_n)) = \sum_i \log(w_i)</script>

<h4 id="the-algorithm-1">The algorithm</h4>

<p>Instead of being that greedy, beam search proposes to maintain <code class="highlighter-rouge">beam_size</code> hypothesis (possible sentences). Then at each step:</p>

<ol>
  <li>We predict the next <code class="highlighter-rouge">beam_size</code> tokens for each hypothesis</li>
  <li>From all those possible sentences we take the <code class="highlighter-rouge">beam_size</code> most probable hypothesis.</li>
</ol>

<p>We can stop when we complete <script type="math/tex">n</script> sentences (arrived at the end character), or after <script type="math/tex">t</script> steps. Additionally, they propose to normalize (to divide) the sentence probability by <script type="math/tex">\alpha</script>, so longer sentences are not less probable.</p>

<h2 id="label-smoothing">Label smoothing</h2>

<p><a href="https://arxiv.org/abs/1512.00567">(Szegedy et. al)</a></p>

<p>This is a regularization technique that encourages the model to be less confident, therefore more adaptable.</p>

<p>In classifications problems we have ground truth data that follows a distribution <script type="math/tex">q</script> that we usually define as a one hot vector:</p>

<script type="math/tex; mode=display">% <![CDATA[
q(y|x_i) = \delta_{y,y_i} := \left\{\begin{array}{ll}1 & \text{if} \quad y=y_i \\0 & \text{if} \quad \text{otherwise} \\\end{array} \right. %]]></script>

<p>If we use the softmax to calculate the output probabilities and we use cross entropy as the loss, then this labels representation can lead to overfitting, therefore this technique proposes to use smoother labels.</p>

<h3 id="understanding-the-problem">Understanding the problem</h3>

<p>In classification problems where we predict the label with a softmax as</p>

<script type="math/tex; mode=display">p(y_j|z) = \frac{\exp(z_j)}{\sum_i \exp(z_i)}</script>

<p>Where <script type="math/tex">x</script> is the input, <script type="math/tex">y_j</script> is one of the possible labels, and <script type="math/tex">z</script> the logits (the output score of our model). And we use the cross entropy loss as shown below for one example (<script type="math/tex">l</script>) and for all of them (<script type="math/tex">L</script>):</p>

<script type="math/tex; mode=display">l = H(q,p) = - \sum_{y=1}^K q(y|x) \log p(y|x)\\\implies L = - \sum_{i=1}^n \sum_{y=1}^K q(y|x_i) \log p(y|x_i)</script>

<p>When we take the ground truth distribution discrete, that is <script type="math/tex">\delta_{y,y_i}</script> (see definition above of <script type="math/tex">q</script>), then <script type="math/tex">q=0</script> for all <script type="math/tex">y</script> different than <script type="math/tex">y_i</script> (the correct label for element <script type="math/tex">i</script>), then:</p>

<script type="math/tex; mode=display">l = - \log p(y_i|x_i)\\</script>

<p>Let‚Äôs now calculate the derivative of this to find the minimum of the loss,</p>

<script type="math/tex; mode=display">\frac{\partial l}{\partial z_k} = \frac{\partial}{\partial z_k}\bigg(-\log \Big(\frac{\exp(z_i)}{\sum_j \exp(z_j)}\Big) \bigg)\\\iff \frac{\partial}{\partial z_k} \bigg( \log \Big(\sum_j \exp(z_j)\Big) - z_i \bigg)\\\iff \frac{1}{\sum_j \exp(z_j)}\frac{\partial}{\partial z_k}\Big(\sum_j \exp(z_j)\Big) - \frac{\partial z_i}{\partial z_k}\\\iff \frac{\exp(z_k)}{\sum_j \exp(z_j)} - \delta_{z_i=z_k}\\\iff p(y_k) - q(y_k)</script>

<p>Thus,</p>

<script type="math/tex; mode=display">\frac{\partial l}{\partial z_k} = p(y_k) - q(y_k) \in [-1, 1]\\</script>

<p>Then the function is minimized (it‚Äôs derivative is zero) when <script type="math/tex">p(y_k) = q(y_k)</script>. Which is approachable if <script type="math/tex">z_i >> z_k \; \forall i \ne k</script>, in words having the correct logit way bigger than the rest. Because with these values the softmax <script type="math/tex">p</script> would output 1 for the <script type="math/tex">i</script> index and zero elsewhere. This can cause two problems:</p>

<ol>
  <li>If the model learns to output <script type="math/tex">z_i >> z_k</script>, then it will be overfitting the groundtruth data and it‚Äôs not guaranteed to generalize.</li>
  <li>It encourages the differences between the largest logit and all others to become large, and this, combined with the fact that the gradient is between -1 and 1 , reduces the ability of the model to adapt. In other words, the model becomes too confident of its predictions.</li>
</ol>

<h3 id="proposed-solution">Proposed solution</h3>

<p>Instead of using a one hot vector, we introduce a noise distribution <script type="math/tex">u(y)</script> on the following way:</p>

<script type="math/tex; mode=display">q'(y|x_i) = (1-\epsilon)\delta_{y,y_i} + \epsilon u(y)</script>

<p>Thus we have a mixture between the old distribution <script type="math/tex">q</script> and the fixed distribution <script type="math/tex">u</script>, with weights <script type="math/tex">(1-\epsilon)</script> and <script type="math/tex">\epsilon</script>. We can see this as in, for a <script type="math/tex">y_j</script> label we first set it to the ground truth label  <script type="math/tex">\delta_{y_j,y_i}</script> and then with probability <script type="math/tex">\epsilon</script> we replace the label with the distribution <script type="math/tex">u</script>.</p>

<p>In the <a href="https://arxiv.org/pdf/1512.00567">paper</a> where this regularization was proposed they used the uniform distribution <script type="math/tex">u(y) = 1/K</script>. If we look at the cross entropy now, it would be:</p>

<script type="math/tex; mode=display">H(q', p) = - \sum_{y=1}^K q'(y|x) \log p(y|x)\\= (1-\epsilon)H(q,p) + \epsilon H(u, p)</script>

<p>Where the term <script type="math/tex">\epsilon H(u, p)</script> is penalising the deviation of <script type="math/tex">p</script> from the prior <script type="math/tex">u</script>, because if these two are too alike, then its cross entropy (<script type="math/tex">H(u, p)</script>) will be bigger and therefore the loss will be bigger.</p>

<h2 id="dropout">Dropout</h2>

<p><a href="http://jmlr.org/papers/v15/srivastava14a.html">(Srivastava et. al)</a></p>

<p>This is another regularization technique that is pretty simple but highly effective. It turns off neurons with a probability <script type="math/tex">(1-p)</script>, or in other words it keeps neurons with a probability <script type="math/tex">p</script>. Doing this the model can learn more relevant patterns and is less prone to overfit, therefore it can achieve better performance.</p>

<p>The intuition behind dropout is that when we delete random neurons, we‚Äôre potentially training exponential sub neural networks at the same time! And then at prediction time, we will be averaging each of those predictions.</p>

<p>In test time we don‚Äôt drop (turn off) neurons, and since it is not feasible to explicitly average the predictions from exponentially many thinned models, we approximate this by multiplying by <script type="math/tex">p</script> the output of the hidden unit. So in this way the expected output of a hidden unit is the same during training and testing.</p>

<h2 id="layer-normalization">Layer Normalization</h2>

<p><a href="https://arxiv.org/pdf/1607.06450.pdf">(Lei Ba et. al 2016)</a></p>

<p>Motivation: During training neural networks converge faster if the input is whitened, that is, linearly transformed to have zero mean and unit variance and decorrelated (<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">LeCun et. al 1998</a>). So we can see the output of one layer as the input of another network, therefore it‚Äôs clear that normalizing the intermediate values in the network could be beneficial.  The main problem is that each layer has to readjust to the changes in the distribution of the input.</p>

<p>This problem was presented by <a href="https://arxiv.org/pdf/1502.03167">Ioffe et. al 2015</a>, where they proposed Batch Normalization to overcome this issue, as a way to normalize the inputs of each layer in the network.</p>

<h3 id="batch-normalization">Batch Normalization</h3>

<p>In a nutshell, we can think of Batch Normalization as an extra layer after each hidden layer, that transforms the inputs for the next layer from <script type="math/tex">x</script> to <script type="math/tex">y</script>. If we consider <script type="math/tex">\mathcal{B} = \{x_1...x_m\}</script> to be the mini batch where each <script type="math/tex">x_j = (x_j^{(1)}...x_j^{(H)})</script> is an input vector of a hidden layer, then the normalization of each dimension is the following:</p>

<script type="math/tex; mode=display">\hat{x}^{(k)} = \frac{x^{(k)} - \text{E}[x^{(k)}]}{\sqrt{\text{Var}(x^{(k)})}}</script>

<p>We‚Äôll approximate the expectation ($\mu$) and the variance (<script type="math/tex">\sigma^2</script>) calculating them at the mini batch level. Then the batch normalization will be:</p>

<script type="math/tex; mode=display">\mu_\mathcal{B} = \frac{1}{m} \sum_{i=1}^m x_i\\\sigma_\mathcal{B}^2 = \frac{1}{m} \sum_{i=1}^m (x_i - \mu_\mathcal{B})\\\hat{x}_i = \frac{x_i - \mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2 + \epsilon}}\\y_i = \gamma \hat{x}_i + \beta \equiv \text{BN}_{\gamma,\beta} (x_i)</script>

<p>Where <script type="math/tex">\epsilon</script> is a constant added for numerical stability, and <script type="math/tex">\gamma</script> and <script type="math/tex">\beta</script> are parameters of this ‚Äúlayer‚Äù, learnt through backpropagation. These parameters are used to be able to keep the representational power of the layer, so by setting <script type="math/tex">\gamma = \sqrt{\sigma^2_\mathcal{B}}</script> and <script type="math/tex">\beta = \mu</script> we can recover the original output, if it were to be the optimal one.</p>

<p>Additionally, during inference we‚Äôll use <script type="math/tex">\gamma</script> and <script type="math/tex">\beta</script> fixed and the expectation and variance will be computed over the entire population (using the first equation).</p>

<h3 id="back-to-layer-normalization">Back to Layer Normalization</h3>

<p>Batch normalization is not easily extendable to Recurrent Neural Networks (RNN), because it requires running averages of the summed input statistics, to compute <script type="math/tex">\mu_\mathcal{B}</script> and <script type="math/tex">\sigma^2_\mathcal{B}</script>. However, the summed inputs in a RNN often vary with the length of the sequence, so applying batch normalization to RNNs appears to require different statistics for different time-steps. Moreover it cannot be applied to online learning tasks (with batch size = 1).</p>

<p>So they proposed layer normalization, that normalises the layers as follows:</p>

<p>Let <script type="math/tex">a^l</script> be the vector of outputs of the <script type="math/tex">l^{\text{th}}</script> hidden layer, and <script type="math/tex">a^l \in R^H</script> (each hidden layer has <script type="math/tex">H</script> hidden units), then:</p>

<script type="math/tex; mode=display">\mu^l = \frac{1}{H} \sum_{i=1}^H a_i^l \qquad \sigma^l = \sqrt{\frac{1}{H}\sum_{i=1}^H (a_i^l - \mu^l)^2}</script>

<p>This looks really similar to the above equations for <script type="math/tex">\mu_\mathcal{B}</script> and <script type="math/tex">\sigma^2_\mathcal{B}</script>, however the equations here use only the hidden layer output whereas the ones above use the whole batch.</p>

<p>Similarly as BN, we‚Äôll learn a linear function (<script type="math/tex">\gamma</script> and <script type="math/tex">\beta</script>) or as they call it in the paper, a gain function <script type="math/tex">g</script>.</p>

<p>Unlike BN, LN is used the same way in training and test times.</p>

<h4 id="comparison-to-bn">Comparison to BN</h4>

<p>In the paper they showed that LN works better (converges faster and it‚Äôs robust to changes in the batch size) for RNNs and feed-forward networks. However BN outperforms LN when applied to CNNs.</p>
:ET